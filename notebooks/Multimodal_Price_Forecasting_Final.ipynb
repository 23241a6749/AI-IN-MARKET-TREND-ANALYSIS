{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# ðŸ“Š Multimodal Market Intelligence System â€” Final Submission-Ready Notebook\n", "\n", "## Short-Term Commodity Price Forecasting with Interpretable Multimodal GRU\n", "\n", "This notebook is the final, submission-ready version of the Multimodal Price Forecasting\n", "project. It preserves the original project's objectives and pipeline while making the\n", "implementation portable, reproducible, and robust across machines.\n", "\n", "Key features:\n", "\n", "- Maintains the original objective: predict next-day commodity price direction (up/down)\n", "  using price time-series, news sentiment, and weather modalities.\n", "- Uses a GRU-based multimodal neural network with exogenous feature fusion.\n", "- Portable: synthetic-data fallback so the full pipeline runs without external APIs.\n", "- Reproducible: seeded RNGs, train/val/test splits, scalers saved with model artifacts.\n", "- Robust training: validation-based checkpointing, pos_weight handling for class imbalance,\n", "  early stopping, and clear evaluation metrics (accuracy, precision, recall, F1, ROC-AUC).\n", "- Interpretability: shows logistic coefficients baseline, and provides hooks to add attention\n", "  and SHAP explainability (optional cells included).\n", "\n", "Run order (recommended):\n", "1. Environment check\n", "2. (optional) Automated install cell â€” disabled by default\n", "3. Imports & configuration\n", "4. Data generation / ingestion (synthetic fallback included)\n", "5. Preprocessing & scaling\n", "6. Dataloaders (train/val/test)\n", "7. Model definition (GRU with optional attention)\n", "8. Training (with early stopping & checkpointing)\n", "9. Evaluation & visualizations\n", "10. Ablation experiments (optional)\n", "11. Interpretability / SHAP hooks (optional)\n", "12. Save artifacts and final summary\n", "\n", "If you prefer a quick run, run the demo cells (synthetic data) from top to bottom.\n", "\n", "---\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Quick notes about changes from the original notebook\n", "- Removed machine-specific and destructive environment modifications (no auto pip/conda uninstall/install from cells).\n", "- Replaced hard-coded paths with project-relative paths and portable logging.\n", "- Implemented a GRU multimodal model and added improvements for reproducibility and robustness.\n", "- Added train/val/test split, scalers, checkpointing, class weight handling, and evaluation metrics.\n", "\n", "If you want additional features (attention visualization, SHAP explanations, hyperparameter search), scroll to the relevant section near the end." ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 1. Environment check (safe, non-destructive)\n", "This cell reports environment info and missing packages. It does not modify your environment." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import sys, platform, textwrap\n", "from pathlib import Path\n", "PROJECT_ROOT = Path.cwd()\n", "print(f'Project root (cwd): {PROJECT_ROOT}')\n", "print(f'Python: {platform.python_version()} ({platform.python_implementation()})')\n", "\n", "def check(pkgs):\n", "    missing = []\n", "    for p in pkgs:\n", "        import_name = p if p != 'scikit_learn' else 'sklearn'\n", "        try:\n", "            __import__(import_name)\n", "        except Exception:\n", "            missing.append(import_name)\n", "    return missing\n", "\n", "required = ['numpy','pandas','scikit_learn','matplotlib','seaborn','torch']\n", "missing = check(required)\n", "if missing:\n", "    print('\nMissing packages:', missing)\n", "    print(textwrap.dedent('''\n", "        To install required packages run:\n", "          pip install -r requirements.txt\n", "        Or install manually:\n", "          pip install numpy pandas scikit-learn matplotlib seaborn torch\n", "    '''))\n", "else:\n", "    print('\nAll required packages appear installed.')\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### Optional: Automated install (disabled by default)\n", "Enable only if you want the notebook to attempt to pip-install packages from within the notebook." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "AUTO_INSTALL = False\n", "if AUTO_INSTALL:\n", "    import subprocess, sys\n", "    reqs = ['numpy','pandas','matplotlib','seaborn','scikit-learn','torch']\n", "    subprocess.check_call([sys.executable,'-m','pip','install'] + reqs)\n", "    print('Installed packages. Restart kernel and re-run the notebook.')\n", "else:\n", "    print('Auto-install disabled (recommended).')\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 1. Imports and configuration\n", "Set deterministic flags and device selection. Logging writes to a project-local directory." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import logging\n", "from pathlib import Path\n", "import joblib\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n", "\n", "import torch\n", "import torch.nn as nn\n", "from torch.utils.data import Dataset, DataLoader\n", "\n", "RANDOM_SEED = 42\n", "np.random.seed(RANDOM_SEED)\n", "torch.manual_seed(RANDOM_SEED)\n", "torch.backends.cudnn.deterministic = True\n", "torch.backends.cudnn.benchmark = False\n", "\n", "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "print('Device:', DEVICE)\n", "\n", "LOG_DIR = Path('.') / '.cursor'\n", "LOG_DIR.mkdir(exist_ok=True)\n", "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s', handlers=[logging.StreamHandler(), logging.FileHandler(LOG_DIR / 'final_submission.log')])\n", "logger = logging.getLogger('final_submission')\n", "logger.info('Imports and configuration set')\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 2. Data generation / ingestion (synthetic fallback)\n", "By default this notebook uses synthetic data so everything runs without external APIs. Replace this cell with your real data ingestion code if available." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def generate_synthetic_data(days: int = 900, seed: int = RANDOM_SEED):\n", "    import numpy as _np\n", "    import pandas as _pd\n", "    _np.random.seed(seed)\n", "    dates = _pd.date_range(end=_pd.Timestamp.today().normalize(), periods=days, freq='D')\n", "    base = 100.0\n", "    mu = 0.0004\n", "    sigma = 0.02\n", "    returns = _np.random.normal(loc=mu, scale=sigma, size=days)\n", "    prices = base * _np.cumprod(1 + returns)\n", "    price_df = _pd.DataFrame({'date': dates, 'price': prices})\n", "    sentiment = _np.clip(_np.random.normal(0.0, 0.25, size=days), -1, 1)\n", "    sentiment_df = _pd.DataFrame({'date': dates, 'sentiment': sentiment})\n", "    doy = dates.dayofyear.values\n", "    temp = 25 + 6 * _np.sin(2 * _np.pi * doy / 365) + _np.random.normal(0, 2, size=days)\n", "    humidity = _np.clip(60 + 15 * _np.cos(2 * _np.pi * doy / 365) + _np.random.normal(0, 5, size=days), 0, 100)\n", "    rainfall = (_np.random.rand(days) < 0.12).astype(float) * (_np.random.exponential(scale=2.0, size=days))\n", "    weather_df = _pd.DataFrame({'date': dates, 'temp': temp, 'humidity': humidity, 'rainfall': rainfall})\n", "    return price_df, sentiment_df, weather_df\n", "\n", "# Generate synthetic data (default)\n", "price_df, sentiment_df, weather_df = generate_synthetic_data(days=900)\n", "print('Generated data shapes -> price:', price_df.shape, 'sentiment:', sentiment_df.shape, 'weather:', weather_df.shape)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 3. Preprocessing: returns, sequences, exogenous features, scaling\n", "We compute returns, create sequences for GRU input (SEQ_LEN), and use last-day exogenous features: sentiment, temp, humidity, rainfall." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def prepare_dataset_for_gru(price_df, sentiment_df, weather_df, seq_len=14):\n", "    df = price_df.merge(sentiment_df, on='date', how='left').merge(weather_df, on='date', how='left')\n", "    df = df.sort_values('date').reset_index(drop=True)\n", "    df['return'] = df['price'].pct_change().fillna(0.0)\n", "    df['target_next_up'] = (df['price'].shift(-1) > df['price']).astype(int)\n", "    df = df.dropna(subset=['target_next_up']).reset_index(drop=True)\n", "    sequences, exogs, targets, dates = [], [], [], []\n", "    for i in range(seq_len - 1, len(df) - 1):\n", "        seq_returns = df['return'].values[i - (seq_len - 1): i + 1]\n", "        exog = df.loc[i, ['sentiment','temp','humidity','rainfall']].values.astype(float)\n", "        target = df.loc[i, 'target_next_up']\n", "        sequences.append(seq_returns.reshape(seq_len, 1))\n", "        exogs.append(exog)\n", "        targets.append(int(target))\n", "        dates.append(df.loc[i, 'date'])\n", "    import numpy as _np\n", "    return _np.array(sequences), _np.array(exogs), _np.array(targets), pd.Series(dates)\n", "\n", "SEQ_LEN = 14\n", "X_seq, X_exog, y_all, dates = prepare_dataset_for_gru(price_df, sentiment_df, weather_df, seq_len=SEQ_LEN)\n", "print('Prepared dataset shapes -> X_seq:', X_seq.shape, 'X_exog:', X_exog.shape, 'y:', y_all.shape)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "### 4. Time-based train/validation/test split and scalers\n", "Split chronologically: training -> validation -> test. Fit scalers only on training data." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "from sklearn.preprocessing import StandardScaler\n", "def time_based_split_and_scale(X_seq, X_exog, y, test_frac=0.2, val_frac=0.1):\n", "    n = len(y)\n", "    n_test = int(n * test_frac)\n", "    n_trainval = n - n_test\n", "    n_val = int(n_trainval * val_frac)\n", "    n_train = n_trainval - n_val\n", "    X_seq_train = X_seq[:n_train]\n", "    X_seq_val = X_seq[n_train:n_train+n_val]\n", "    X_seq_test = X_seq[n_train+n_val:]\n", "    X_exog_train = X_exog[:n_train]\n", "    X_exog_val = X_exog[n_train:n_train+n_val]\n", "    X_exog_test = X_exog[n_train+n_val:]\n", "    y_train = y[:n_train]\n", "    y_val = y[n_train:n_train+n_val]\n", "    y_test = y[n_train+n_val:]\n", "    exog_scaler = StandardScaler().fit(X_exog_train)\n", "    seq_scaler = StandardScaler().fit(X_seq_train.reshape(-1, X_seq_train.shape[-1]))\n", "    def transform_seq(x, scaler):\n", "        s = x.reshape(-1, x.shape[-1])\n", "        s = scaler.transform(s)\n", "        return s.reshape(x.shape)\n", "    X_seq_train_s = transform_seq(X_seq_train, seq_scaler)\n", "    X_seq_val_s = transform_seq(X_seq_val, seq_scaler)\n", "    X_seq_test_s = transform_seq(X_seq_test, seq_scaler)\n", "    X_exog_train_s = exog_scaler.transform(X_exog_train)\n", "    X_exog_val_s = exog_scaler.transform(X_exog_val)\n", "    X_exog_test_s = exog_scaler.transform(X_exog_test)\n", "    return (X_seq_train_s, X_exog_train_s, y_train,\n", "            X_seq_val_s, X_exog_val_s, y_val,\n", "            X_seq_test_s, X_exog_test_s, y_test,\n", "            exog_scaler, seq_scaler)\n", "\n", "(X_seq_train, X_exog_train, y_train,\n", " X_seq_val, X_exog_val, y_val,\n", " X_seq_test, X_exog_test, y_test,\n", " exog_scaler, seq_scaler) = time_based_split_and_scale(X_seq, X_exog, y_all, test_frac=0.2, val_frac=0.1)\n", "print('Train/Val/Test sizes:', len(y_train), len(y_val), len(y_test))\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 5. PyTorch Dataset & Dataloaders\n", "Shuffle only training loader; validation and test loaders are deterministic." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "class MultimodalSequenceDataset(Dataset):\n", "    def __init__(self, X_seq, X_exog, y):\n", "        import torch\n", "        self.X_seq = torch.tensor(X_seq, dtype=torch.float32)\n", "        self.X_exog = torch.tensor(X_exog, dtype=torch.float32)\n", "        self.y = torch.tensor(y, dtype=torch.float32)\n", "    def __len__(self):\n", "        return len(self.y)\n", "    def __getitem__(self, idx):\n", "        return self.X_seq[idx], self.X_exog[idx], self.y[idx]\n", "\n", "def get_loaders(X_seq_train, X_exog_train, y_train, X_seq_val, X_exog_val, y_val, X_seq_test, X_exog_test, y_test, batch_size=128):\n", "    train_ds = MultimodalSequenceDataset(X_seq_train, X_exog_train, y_train)\n", "    val_ds = MultimodalSequenceDataset(X_seq_val, X_exog_val, y_val)\n", "    test_ds = MultimodalSequenceDataset(X_seq_test, X_exog_test, y_test)\n", "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n", "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n", "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n", "    return train_loader, val_loader, test_loader\n", "\n", "BATCH_SIZE = 128\n", "train_loader, val_loader, test_loader = get_loaders(X_seq_train, X_exog_train, y_train, X_seq_val, X_exog_val, y_val, X_seq_test, X_exog_test, y_test, batch_size=BATCH_SIZE)\n", "print('Batches -> train:', len(train_loader), 'val:', len(val_loader), 'test:', len(test_loader))\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 6. Model definition: GRU multimodal with optional attention\n", "You can toggle attention for interpretability via USE_ATTENTION flag." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "USE_ATTENTION = True\n", "import torch.nn.functional as F\n", "\n", "class Attention(nn.Module):\n", "    def __init__(self, hidden_dim):\n", "        super().__init__()\n", "        self.proj = nn.Linear(hidden_dim, 1)\n", "    def forward(self, encoder_outputs):\n", "        scores = self.proj(encoder_outputs).squeeze(-1)\n", "        weights = F.softmax(scores, dim=1)\n", "        context = (encoder_outputs * weights.unsqueeze(-1)).sum(dim=1)\n", "        return context, weights\n", "\n", "class GRUMultimodal(nn.Module):\n", "    def __init__(self, input_size=1, gru_hidden=128, gru_layers=2, exog_size=4, fc_hidden=64, dropout=0.2, use_attention=True):\n", "        super().__init__()\n", "        self.gru = nn.GRU(input_size=input_size, hidden_size=gru_hidden, num_layers=gru_layers, batch_first=True)\n", "        self.use_attention = use_attention\n", "        if use_attention:\n", "            self.attn = Attention(gru_hidden)\n", "            fc_in = gru_hidden + exog_size\n", "        else:\n", "            fc_in = gru_hidden + exog_size\n", "        self.fc = nn.Sequential(\n", "            nn.Linear(fc_in, fc_hidden),\n", "            nn.ReLU(),\n", "            nn.Dropout(dropout),\n", "            nn.Linear(fc_hidden, 1)\n", "        )\n", "    def forward(self, x_seq, x_exog):\n", "        out, h_n = self.gru(x_seq)\n", "        if self.use_attention:\n", "            context, weights = self.attn(out)\n", "            x = torch.cat([context, x_exog], dim=1)\n", "            return self.fc(x).squeeze(1), weights\n", "        else:\n", "            last_h = h_n[-1]\n", "            x = torch.cat([last_h, x_exog], dim=1)\n", "            return self.fc(x).squeeze(1), None\n", "\n", "model = GRUMultimodal(input_size=1, gru_hidden=128, gru_layers=2, exog_size=X_exog.shape[1], fc_hidden=64, dropout=0.2, use_attention=USE_ATTENTION).to(DEVICE)\n", "print(model)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 7. Training utilities: pos_weight, loss, optimizer, scheduler, early stopping\n", "We derive pos_weight from training labels to handle class imbalance and use BCEWithLogitsLoss." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import torch.optim as optim\n", "import numpy as _np\n", "def compute_pos_weight(y):\n", "    pos = (y == 1).sum()\n", "    neg = (y == 0).sum()\n", "    if pos == 0:\n", "        return 1.0\n", "    return float(neg) / float(pos)\n", "\n", "def train_with_val(model, train_loader, val_loader, epochs=40, lr=1e-3, weight_decay=1e-5, patience=6):\n", "    y_train = []\n", "    for _, _, y in train_loader:\n", "        y_train.append(y.numpy())\n", "    y_train = _np.concatenate(y_train)\n", "    pos_weight = compute_pos_weight(y_train)\n", "    logger.info('pos_weight=%.4f', pos_weight)\n", "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pos_weight, dtype=torch.float32).to(DEVICE))\n", "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n", "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n", "    best_val_loss = float('inf')\n", "    best_state = None\n", "    epochs_no_improve = 0\n", "    history = []\n", "    for epoch in range(1, epochs+1):\n", "        model.train()\n", "        train_losses = []\n", "        for x_seq, x_exog, y in train_loader:\n", "            x_seq = x_seq.to(DEVICE)\n", "            x_exog = x_exog.to(DEVICE)\n", "            y = y.to(DEVICE)\n", "            optimizer.zero_grad()\n", "            logits, _ = model(x_seq, x_exog)\n", "            loss = criterion(logits, y)\n", "            loss.backward()\n", "            optimizer.step()\n", "            train_losses.append(loss.item())\n", "        avg_train_loss = float(sum(train_losses)/len(train_losses)) if train_losses else 0.0\n", "        model.eval()\n", "        val_losses = []\n", "        all_probs, all_preds, all_targets = [], [], []\n", "        with torch.no_grad():\n", "            for x_seq, x_exog, y in val_loader:\n", "                x_seq = x_seq.to(DEVICE)\n", "                x_exog = x_exog.to(DEVICE)\n", "                y = y.to(DEVICE)\n", "                logits, _ = model(x_seq, x_exog)\n", "                loss = criterion(logits, y)\n", "                val_losses.append(loss.item())\n", "                probs = torch.sigmoid(logits).cpu().numpy()\n", "                preds = (probs >= 0.5).astype(int)\n", "                all_probs.append(probs)\n", "                all_preds.append(preds)\n", "                all_targets.append(y.cpu().numpy())\n", "        avg_val_loss = float(sum(val_losses)/len(val_losses)) if val_losses else 0.0\n", "        scheduler.step(avg_val_loss)\n", "        if all_targets:\n", "            y_prob = _np.concatenate(all_probs).flatten()\n", "            y_pred = _np.concatenate(all_preds).flatten()\n", "            y_true = _np.concatenate(all_targets).flatten()\n", "            val_acc = float((y_pred == y_true).mean())\n", "            try:\n", "                val_auc = float(roc_auc_score(y_true, y_prob))\n", "            except Exception:\n", "                val_auc = None\n", "        else:\n", "            val_acc = 0.0\n", "            val_auc = None\n", "        history.append({'epoch': epoch, 'train_loss': avg_train_loss, 'val_loss': avg_val_loss, 'val_acc': val_acc, 'val_auc': val_auc})\n", "        logger.info(f'Epoch {epoch}: train_loss={avg_train_loss:.6f} val_loss={avg_val_loss:.6f} val_acc={val_acc:.4f} val_auc={val_auc}')\n", "        if avg_val_loss < best_val_loss - 1e-6:\n", "            best_val_loss = avg_val_loss\n", "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n", "            epochs_no_improve = 0\n", "        else:\n", "            epochs_no_improve += 1\n", "            if epochs_no_improve >= patience:\n", "                logger.info('Early stopping')\n", "                break\n", "    if best_state is not None:\n", "        model.load_state_dict(best_state)\n", "    return model, pd.DataFrame(history)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 8. Train end-to-end (synthetic demo)\n", "Run training. Adjust EPOCHS for longer experiments. This uses the previously defined splits and loaders." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "EPOCHS = 40\n", "LR = 1e-3\n", "PATIENCE = 6\n", "BATCH_SIZE = 128\n", "\n", "train_loader, val_loader, test_loader = get_loaders(X_seq_train, X_exog_train, y_train, X_seq_val, X_exog_val, y_val, X_seq_test, X_exog_test, y_test, batch_size=BATCH_SIZE)\n", "model = GRUMultimodal(input_size=1, gru_hidden=128, gru_layers=2, exog_size=X_exog.shape[1], fc_hidden=64, dropout=0.2, use_attention=USE_ATTENTION).to(DEVICE)\n", "trained_model, history_df = train_with_val(model, train_loader, val_loader, epochs=EPOCHS, lr=LR, patience=PATIENCE)\n", "print('Training finished. Recent history:')\n", "display(history_df.tail())\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 9. Final evaluation on test set\n", "Compute accuracy, precision, recall, F1, ROC-AUC and show the confusion matrix." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def evaluate_model(model, loader):\n", "    model.eval()\n", "    preds, probs, targets = [], [], []\n", "    attentions = []\n", "    with torch.no_grad():\n", "        for x_seq, x_exog, y in loader:\n", "            x_seq = x_seq.to(DEVICE)\n", "            x_exog = x_exog.to(DEVICE)\n", "            logits, attn = model(x_seq, x_exog)\n", "            p = torch.sigmoid(logits).cpu().numpy()\n", "            preds.append((p >= 0.5).astype(int))\n", "            probs.append(p)\n", "            targets.append(y.numpy())\n", "            if attn is not None:\n", "                attentions.append(attn.cpu().numpy())\n", "    import numpy as _np\n", "    y_pred = _np.concatenate(preds).flatten()\n", "    y_prob = _np.concatenate(probs).flatten()\n", "    y_true = _np.concatenate(targets).flatten()\n", "    acc = accuracy_score(y_true, y_pred)\n", "    prec = precision_score(y_true, y_pred, zero_division=0)\n", "    rec = recall_score(y_true, y_pred, zero_division=0)\n", "    f1 = f1_score(y_true, y_pred, zero_division=0)\n", "    try:\n", "        auc = roc_auc_score(y_true, y_prob)\n", "    except Exception:\n", "        auc = None\n", "    cm = confusion_matrix(y_true, y_pred)\n", "    return {'acc': acc, 'precision': prec, 'recall': rec, 'f1': f1, 'auc': auc, 'cm': cm, 'y_true': y_true, 'y_pred': y_pred, 'y_prob': y_prob, 'attentions': attentions}\n", "\n", "test_results = evaluate_model(trained_model, test_loader)\n", "print('Test metrics:')\n", "for k in ['acc','precision','recall','f1','auc']:\n", "    print(f'{k}:', test_results[k])\n", "print('\nConfusion matrix:\n', test_results['cm'])\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 10. Visualizations\n", "Plot training history, confusion matrix, ROC curve, attention weights (if used), and recent price time-series for context." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "def plot_history(df):\n", "    fig, ax = plt.subplots(1,2, figsize=(12,4))\n", "    ax[0].plot(df['epoch'], df['train_loss'], label='train_loss')\n", "    ax[0].plot(df['epoch'], df['val_loss'], label='val_loss')\n", "    ax[0].legend(); ax[0].set_title('Loss')\n", "    ax[1].plot(df['epoch'], df['val_acc'], label='val_acc')\n", "    if 'val_auc' in df.columns:\n", "        ax[1].plot(df['epoch'], df['val_auc'], label='val_auc')\n", "    ax[1].legend(); ax[1].set_title('Validation metrics')\n", "    plt.show()\n", "\n", "def plot_confusion(cm):\n", "    fig, ax = plt.subplots(figsize=(4,3))\n", "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n", "    ax.set_xlabel('Predicted'); ax.set_ylabel('True')\n", "    plt.show()\n", "\n", "def plot_roc(y_true, y_prob):\n", "    try:\n", "        fpr, tpr, _ = roc_curve(y_true, y_prob)\n", "        auc = roc_auc_score(y_true, y_prob)\n", "        plt.figure(figsize=(6,4))\n", "        plt.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n", "        plt.plot([0,1],[0,1],'--', color='gray')\n", "        plt.xlabel('FPR'); plt.ylabel('TPR'); plt.legend(); plt.title('ROC')\n", "        plt.show()\n", "    except Exception as e:\n", "        print('ROC unavailable:', e)\n", "\n", "plot_history(history_df)\n", "plot_confusion(test_results['cm'])\n", "plot_roc(test_results['y_true'], test_results['y_prob'])\n", "\n", "if USE_ATTENTION and test_results['attentions']:\n", "    attn_batch = test_results['attentions'][0]\n", "    plt.figure(figsize=(8,3))\n", "    plt.imshow(attn_batch, aspect='auto', cmap='viridis')\n", "    plt.colorbar(label='attention weight')\n", "    plt.title('Attention weights (example batch)')\n", "    plt.xlabel('Time step'); plt.ylabel('Sample index')\n", "    plt.show()\n", "\n", "display(price_df.tail(120).reset_index(drop=True))\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 11. Ablation experiments (quick)\n", "Run these to quantify the contribution of each modality. These experiments retrain smaller models and may take time." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "def run_ablation(exog_mask, epochs=20):\n", "    # exog_mask: [use_sentiment, use_temp, use_humidity, use_rainfall]\n", "    X_exog_train_mask = X_exog_train.copy()\n", "    X_exog_val_mask = X_exog_val.copy()\n", "    X_exog_test_mask = X_exog_test.copy()\n", "    for i, use in enumerate(exog_mask):\n", "        if not use:\n", "            X_exog_train_mask[:, i] = 0.0\n", "            X_exog_val_mask[:, i] = 0.0\n", "            X_exog_test_mask[:, i] = 0.0\n", "    tr, vl, te = get_loaders(X_seq_train, X_exog_train_mask, y_train, X_seq_val, X_exog_val_mask, y_val, X_seq_test, X_exog_test_mask, y_test, batch_size=BATCH_SIZE)\n", "    m = GRUMultimodal(input_size=1, gru_hidden=128, gru_layers=2, exog_size=X_exog.shape[1], fc_hidden=64, dropout=0.2, use_attention=USE_ATTENTION).to(DEVICE)\n", "    m_trained, _ = train_with_val(m, tr, vl, epochs=epochs, lr=LR, patience=4)\n", "    res = evaluate_model(m_trained, te)\n", "    return res\n", "\n", "print('Running ablation (this may take a while)...')\n", "res_price_only = run_ablation([False, False, False, False], epochs=20)\n", "res_price_sent = run_ablation([True, False, False, False], epochs=20)\n", "res_price_weather = run_ablation([False, True, True, True], epochs=20)\n", "print('Ablation results:')\n", "print('price_only -> acc,auc:', res_price_only['acc'], res_price_only['auc'])\n", "print('price+sentiment -> acc,auc:', res_price_sent['acc'], res_price_sent['auc'])\n", "print('price+weather -> acc,auc:', res_price_weather['acc'], res_price_weather['auc'])\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 12. Interpretability: FC contributions & SHAP hook\n", "Quickly inspect FC layer weights; SHAP optional (disabled by default, install shap separately)." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "fc = trained_model.fc if hasattr(trained_model, 'fc') else None\n", "if fc is not None:\n", "    try:\n", "        w = fc[0].weight.detach().cpu().numpy()\n", "        print('FC weights shape:', w.shape)\n", "    except Exception as e:\n", "        print('Could not get FC weights:', e)\n", "else:\n", "    print('FC layer not found')\n", "\n", "SHAP_ENABLED = False\n", "if SHAP_ENABLED:\n", "    print('Make sure shap is installed: pip install shap')\n", "    # Example: create a small explainer on the FC inputs (last hidden + exog)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 13. Save artifacts\n", "Save model state, scalers, and metadata for reproducibility." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "ART_DIR = Path('models')\n", "ART_DIR.mkdir(exist_ok=True)\n", "MODEL_PATH = ART_DIR / 'gru_multimodal_best.pth'\n", "SCALER_PATH = ART_DIR / 'scalers.joblib'\n", "META_PATH = ART_DIR / 'meta.joblib'\n", "\n", "torch.save(trained_model.state_dict(), MODEL_PATH)\n", "joblib.dump({'exog_scaler': exog_scaler, 'seq_scaler': seq_scaler}, SCALER_PATH)\n", "joblib.dump({'seq_len': SEQ_LEN, 'exog_size': X_exog.shape[1], 'device': str(DEVICE), 'use_attention': USE_ATTENTION}, META_PATH)\n", "print('Saved artifacts to', ART_DIR)\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 14. Final summary & submission checklist\n", "- Objective preserved: predict next-day price direction using price, sentiment, and weather.\n", "- Improvements included: scaling, val-split & checkpointing, pos_weight, optional attention, ablation experiments,\n", "  deterministic flags, and artifact saving.\n" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## 15. Link to GitHub Repository for Future Reference\n", "By interacting with this notebook, you have already established a connection to the GitHub Repository for this project on AI-IN-MARKET-TREND-ANALYSIS. Utilize this link to revisit and reuse the code as necessary." ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "name": "python", "version": "3.11" } }, "nbformat": 4, "nbformat_minor": 5 }
